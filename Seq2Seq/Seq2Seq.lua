--Creates a Seq2Seq model that is used to spell check sentences.

require 'cunn'
require 'rnn'
require 'optim'

local Seq2Seq = {}

local logger = optim.Logger('train.log')
logger:setNames { 'loss' }
logger:style { '-' }

function Seq2Seq:init(params)
    self.lr = params.learningRate
    self.iterations = params.iterations
    self.vocabSize = params.vocabSize --The size of the alphabet.
    self.seqlen = params.seqlen -- length of the encoded sequence
    self.nbOfHiddenLayers = params.nbOfHiddenLayers -- Number of hidden layers of LSTMS excluding encoder-decoder layers.
    self.hiddenLayerSize = params.hiddenLayerSize -- hidden layer sizes including encoder-decoder layers.
    self.maxLabelSize = params.maxLabelSize -- The maximum characters if end label not generated by decoder.
    -- Encoder
    local enc = nn.Sequential()
    enc:add(nn.LookupTable(Seq2Seq.vocabSize, Seq2Seq.hiddenLayerSize))
    enc:add(nn.SplitTable(1, 2))

    --Add all hidden encoder LSTM layers.
    for x = 1, Seq2Seq.nbOfHiddenLayers do
        enc:add(nn.Sequencer(nn.Dropout(0.2)))
        enc:add(nn.Sequencer(nn.LSTM(Seq2Seq.hiddenLayerSize, Seq2Seq.hiddenLayerSize)))
    end
    --Add hidden encoder LSTM layer.
    local encLSTM = nn.LSTM(Seq2Seq.hiddenLayerSize, Seq2Seq.hiddenLayerSize)
    self.encLSTM = encLSTM
    enc:add(nn.Sequencer(nn.Dropout(0.2)))
    enc:add(nn.Sequencer(encLSTM))
    enc:add(nn.SelectTable(-1))
    enc:cuda()

    -- Decoder
    local dec = nn.Sequential()
    dec:add(nn.LookupTable(Seq2Seq.vocabSize, Seq2Seq.hiddenLayerSize))
    dec:add(nn.SplitTable(1, 2)) -- works for both online and mini-batch mode
    --Add hidden decoder LSTM layer.
    local decLSTM = nn.LSTM(Seq2Seq.hiddenLayerSize, Seq2Seq.hiddenLayerSize)
    self.decLSTM = decLSTM
    dec:add(nn.Sequencer(decLSTM))

    --Add all hidden LSTM layers.
    for x = 1, Seq2Seq.nbOfHiddenLayers do
        dec:add(nn.Sequencer(nn.Dropout(0.2)))
        dec:add(nn.Sequencer(nn.LSTM(Seq2Seq.hiddenLayerSize, Seq2Seq.hiddenLayerSize)))
    end
    dec:add(nn.Sequencer(nn.Dropout(0.2)))
    dec:add(nn.Sequencer(nn.Linear(Seq2Seq.hiddenLayerSize, Seq2Seq.vocabSize)))
    dec:add(nn.Sequencer(nn.LogSoftMax()))
    dec:cuda()
    local criterion = nn.SequencerCriterion(nn.ClassNLLCriterion()):cuda()
    self.enc = enc
    self.dec = dec
    self.criterion = criterion
end

--[[ Forward coupling: Copy encoder cell and output to decoder LSTM ]] --
local function forwardConnect(encLSTM, decLSTM)
    decLSTM.userPrevOutput = nn.rnn.recursiveCopy(decLSTM.userPrevOutput, encLSTM.outputs[Seq2Seq.seqlen])
    decLSTM.userPrevCell = nn.rnn.recursiveCopy(decLSTM.userPrevCell, encLSTM.cells[Seq2Seq.seqlen])
end

--[[ Backward coupling: Copy decoder gradients to encoder LSTM ]] --
local function backwardConnect(encLSTM, decLSTM)
    encLSTM.userNextGradCell = nn.rnn.recursiveCopy(encLSTM.userNextGradCell, decLSTM.userGradPrevCell)
    encLSTM.gradPrevOutput = nn.rnn.recursiveCopy(encLSTM.gradPrevOutput, decLSTM.userGradPrevOutput)
end

function Seq2Seq:trainNetwork(dataset)
    for i = 1, Seq2Seq.iterations do
        Seq2Seq.enc:zeroGradParameters()
        Seq2Seq.dec:zeroGradParameters()

        local encInSeq, decInSeq, decOutSeq = dataset:nextData()
        local decOutSeq = nn.SplitTable(1, 1):forward(decOutSeq:clone())

        -- Forward pass
        local encOut = Seq2Seq.enc:forward(encInSeq)

        forwardConnect(Seq2Seq.encLSTM, Seq2Seq.decLSTM)
        local decOut = Seq2Seq.dec:forward(decInSeq)
        local err = Seq2Seq.criterion:forward(decOut, decOutSeq)
        print("Loss: ", err, " iteration: ", i)
        logger:add { err }
        -- Backward pass
        local gradOutput = Seq2Seq.criterion:backward(decOut, decOutSeq)
        Seq2Seq.dec:backward(decInSeq, gradOutput)
        backwardConnect(Seq2Seq.encLSTM, Seq2Seq.decLSTM)
        local zeroTensor = torch.Tensor(2):zero():cuda()
        Seq2Seq.enc:backward(encInSeq, zeroTensor)

        Seq2Seq.dec:updateParameters(Seq2Seq.lr)
        Seq2Seq.enc:updateParameters(Seq2Seq.lr)
    end
end

--Finds the index of the max value in the 1d table given. Used when retrieving the prediction from the network.
local function maxIndex(table)
    local maxIndex = 1
    local maxValue = table[1]
    for index, value in ipairs(table) do
        if (value > maxValue) then maxValue = value maxIndex = index end
    end
    return maxIndex
end

function Seq2Seq:predictNetwork(input, labels)
    -- Forward pass
    local encOut = Seq2Seq.enc:forward(input)
    forwardConnect(Seq2Seq.encLSTM, Seq2Seq.decLSTM)

    local predictions = {}
    local currentPrediction = labels.startLabel
    local numberOfPredictions = 0
    while currentPrediction ~= labels.endLabel and numberOfPredictions < self.maxLabelSize do
        local decOut = Seq2Seq.dec:forward(torch.Tensor({ { currentPrediction } }))
        local max = maxIndex(torch.totable(decOut[1])[1])
        currentPrediction = max
        numberOfPredictions = numberOfPredictions + 1
        table.insert(predictions, currentPrediction)
    end
    return predictions
end

function Seq2Seq:createLossGraph()
    logger:plot()
    while true do end
end

return Seq2Seq